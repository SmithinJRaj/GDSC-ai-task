import torch
import torch.nn as nn
from torch.nn import functional as F

# -*- coding: utf-8 -*-
"""MyGPT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r3LDTJhymgd6pYynpdfVS8AkZ5-3ig2R
"""

# !wget https://raw.githubusercontent.com/karpathy/ng-video-lecture/refs/heads/master/input.txt -O input.txt

batchSize = 32
blockSize = 8
max_iters = 3000
eval_interval = 300
learning_rate = 1e-2
device = 'cuda' if torch.cuda.is_available() else 'cpu'
eval_iters = 200
torch.manual_seed(1337)

with open('input.txt', 'r',encoding='utf-8') as f:
    text = f.read();

setOfLetters = set(text)
listOfCharacters = list(setOfLetters)
sortedList = sorted(listOfCharacters)

vocabSize = len(sortedList)

i = 0;
stoi = {}
for ch in sortedList:
  stoi[ch] = i;
  i=i+1
i = 0;
itos = {}
for ch in sortedList:
  itos[i] = ch;
  i=i+1

def encode(s):
    l = [];
    for ch in s:
      l.append(stoi[ch])
    return l

def decode(nums):
    # nums: Tensor of shape (T,) or list of ints
    if isinstance(nums, torch.Tensor):
        nums = nums.tolist()

    s = ""
    for num in nums:
        s += itos[int(num)]
    return s

data = torch.tensor(encode(text), dtype=torch.long)

n = int(0.9*len(data))
trainData = data[:n]
valData = data[n:]

trainData[:blockSize+1]

def getBatch(split):
  data = trainData if split == 'train' else valData
  ix = torch.randint(len(data) - blockSize, (batchSize,))
  x = torch.stack([data[i:i+blockSize] for i in ix])
  y = torch.stack([data[i+1:i+blockSize+1] for i in ix])
  x,y = x.to(device), y.to(device)
  return x,y

@torch.no_grad()
def estimateLoss():
  out = {}
  model.eval()
  for split in ['train','val']:
    losses = torch.zeros(eval_iters)
    for k in range(eval_iters):
      X, Y = getBatch(split)
      logits,loss = model(X,Y)
      losses[k] = loss.item()
    out[split] = losses.mean()
  model.train()
  return out

class bigramLanguageModel(nn.Module):
    def __init__(self,vocabSize):
      super().__init__()
      self.token_embedding_table = nn.Embedding(vocabSize,vocabSize)

    def forward(self, idx, targets=None):
      logits = self.token_embedding_table(idx)

      if targets is None:
        loss = None
      else:
        B, T, C = logits.shape
        logits = logits.view(B*T, C)
        targets = targets.view(B*T)
        loss = F.cross_entropy(logits, targets)

      return logits, loss

    def generate(self, idx, maxNewTokens):
      for _ in range(maxNewTokens):
        logits, loss = self(idx)
        logits = logits[:, -1, :]
        probs = F.softmax(logits, dim=-1)
        idxNext = torch.multinomial(probs, num_samples=1)
        idx = torch.cat((idx, idxNext), dim = 1)
      return idx

model = bigramLanguageModel(vocabSize)
m = model.to(device)

optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

for iter in range(max_iters):
  if iter % eval_interval == 0:
    losses = estimateLoss()
    print(f"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}")
  
  xb, yb = getBatch('train')

  logits, loss = m(xb, yb)
  optimizer.zero_grad(set_to_none=True)
  loss.backward()
  optimizer.step()

  print(loss.item())

context = torch.zeros((1, 1), dtype=torch.long, device=device)
out = m.generate(context, maxNewTokens=500)
print(decode(out[0]))
